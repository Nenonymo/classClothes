{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove all mentions of blouse_DF and sunderas before writing to CSV, directly after making dataframe\n",
    "# Another approach is to remove the JSON files of everything not MORIA.... and the MIROA with blouse_DF etc.\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize \n",
    "from pandasql import sqldf\n",
    "import numpy as np\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "DATAPATH = \"../../data/Datasets_Enhancy/Annotation/\"\n",
    "IMAGEPATH = \"../../data/Datasets_Enhancy/datasets/\"\n",
    "\n",
    "onlyfiles = [f for f in listdir(DATAPATH) if isfile(join(DATAPATH, f))]\n",
    "ret = []\n",
    "for f in onlyfiles:\n",
    "    #if (\"MIROA\") in f and not (\"blouse\" in f or \"sudaderas\" in f or \"top_DF\" in f):\n",
    "    ret.append(f)\n",
    "print(ret)\n",
    "frames =[]\n",
    "for i in range(len(ret)):\n",
    "    with open(DATAPATH + ret[i]) as f:\n",
    "        d = json.load(f)\n",
    "        frames.append(pd.json_normalize(d['annotations']))\n",
    "        \n",
    "df = pd.concat(frames)\n",
    "exportDf = df[[i for i in df.columns if \"image_id\" in i or \"attributes\" in i]]\n",
    "exportDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deleted = 0\n",
    "changed = 0\n",
    "already_dropped = 0\n",
    "s = len(exportDf.index)\n",
    "outPut = exportDf.copy()\n",
    "\n",
    "for i in range(s):\n",
    "    id = exportDf.image_id.iloc[i]\n",
    "    path1 = IMAGEPATH + id\n",
    "    path2 = IMAGEPATH + \"dataset/\" + id\n",
    "    path3 = IMAGEPATH + \"Luxury_shops/\" + id\n",
    "\n",
    "    \n",
    "    if not os.path.isfile(path1):\n",
    "        if not os.path.isfile(path2):\n",
    "            if not os.path.isfile(path3):\n",
    "                try:\n",
    "                    idx = outPut.loc[outPut['image_id'] == id].index[0]\n",
    "                    print(\"DROPPED AT idx {} path {}\".format(i, path2))\n",
    "                    outPut = outPut.drop(idx, axis = 0)\n",
    "                    deleted += 1\n",
    "                except:\n",
    "                    print(\"ALREADY DROPPED {}\".format(path2))\n",
    "                    already_dropped += 1\n",
    "            \n",
    "newLen = len(outPut.index)\n",
    "for i in range(newLen):\n",
    "    id = outPut.image_id.iloc[i]\n",
    "    if os.path.isfile(IMAGEPATH + \"dataset/\" + id):\n",
    "        outPut.loc[outPut['image_id'] == id, \"image_id\"] = \"dataset/\" + id\n",
    "    elif os.path.isfile(IMAGEPATH + \"Luxury_shops/\" + id):\n",
    "        outPut.loc[outPut['image_id'] == id, \"image_id\"] = \"Luxury_shops/\" + id\n",
    "\n",
    "outPut[\"id\"] = np.arange(len(outPut))\n",
    "print(\"Total: {}\".format(s))    \n",
    "print(\"Deleted: {}\".format(deleted))\n",
    "print(\"Already Deleted: {}\".format(already_dropped))\n",
    "print(\"Edited: {}\".format(changed))\n",
    "#outPut.to_csv(r'./export_data.csv', index = False)\n",
    "outPut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dicts which specifies valid entries for each type of attribute\n",
    "# Create masterlist with all types of attributes\n",
    "attribute_types = {}\n",
    "attribute_map = {}\n",
    "i = 0\n",
    "for col in cols:\n",
    "    vals = df[col].drop_duplicates().tolist()\n",
    "    attribute_types[col] = vals\n",
    "    attribute_map[col] = i\n",
    "    i += 1\n",
    "print(attribute_map)\n",
    "print(attribute_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newLabel = []\n",
    "for index, row in exportDf.iterrows():\n",
    "    attr = row[\"attributes\"]\n",
    "    res = []\n",
    "    for t in attribute_types:\n",
    "        possible = attribute_types[t]\n",
    "        for val in possible:\n",
    "            print(val)\n",
    "            if \"Col_Classique\" in val:\n",
    "                print(\"TEST\")\n",
    "                break\n",
    "            if not \"Null\" in val and val in attr:\n",
    "                break\n",
    "                res.append(val)\n",
    "            else:\n",
    "                res.append(\"Null\")\n",
    "    newLabel.append(res)\n",
    "                \n",
    "    break\n",
    "print(newLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSVPATH = r\"../../data/csv/export_labels_working.csv\"\n",
    "outPut.to_csv(CSVPATH, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
